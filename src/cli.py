#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import argparse
from llama_cpp import Llama
import time
import subprocess # For nvidia-smi (optional)

# --- Model Loading Function ---
# Loads the model based on arguments
def load_model(args):
    print("â„¹ï¸ GPUåˆ©ç”¨ã®ç¢ºèªä¸­...")
    # Optional: Check nvidia-smi availability/output
    # try:
    #     subprocess.run(["nvidia-smi"], check=True)
    # except Exception as e:
    #     print(f"âš ï¸ nvidia-smi check failed: {e}")

    # --- Use the model path specified in args ---
    model_path_to_load = args.model
    # ------------------------------------------

    print(f"â„¹ï¸ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {model_path_to_load}")
    print(f"â„¹ï¸ GPUå±¤æ•°: {args.n_gpu_layers}, ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: {args.n_ctx}, ã‚¹ãƒ¬ãƒƒãƒ‰æ•°: {args.n_threads}")

    start_time = time.time()
    try:
        llm = Llama(
            model_path=model_path_to_load, # Use the determined path
            n_gpu_layers=args.n_gpu_layers,
            n_ctx=args.n_ctx,
            n_threads=args.n_threads,
            verbose=args.verbose
        )
        load_time = time.time() - start_time
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸ (æ‰€è¦æ™‚é–“: {load_time:.2f}ç§’)")
        # Optional: Check nvidia-smi after load
        # try:
        #     subprocess.run(["nvidia-smi"], check=True)
        # except Exception as e:
        #     print(f"âš ï¸ nvidia-smi check failed: {e}")
        return llm
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print("âŒ VRAMä¸è¶³ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚n_gpu_layersã‚’æ¸›ã‚‰ã™ã‹ã€ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1) # Exit if model loading fails

# --- Global Variables ---
conversation_history = []
llm_instance = None # Global variable to hold the loaded model

# --- Argument Parsing ---
# Create parser once
parser = argparse.ArgumentParser(description='LLaMA-CPP Pythonã‚’ä½¿ç”¨ã—ãŸCLIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³')
parser.add_argument('--model', type=str, default='/app/models/Berghof-NSFW-7B.i1-IQ4_XS.gguf', # Keep user's default
                    help='ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
# --- Change n_gpu_layers default to -1 ---
parser.add_argument('--n_gpu_layers', type=int, default=-1, # Default to use all GPU layers
                    help='GPUã«å‰²ã‚Šå½“ã¦ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•° (-1ã§ã™ã¹ã¦)')
# -----------------------------------------
parser.add_argument('--n_ctx', type=int, default=2048,
                    help='ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚º')
parser.add_argument('--n_threads', type=int, default=None, # Let llama-cpp decide
                    help='ã‚¹ãƒ¬ãƒƒãƒ‰æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: è‡ªå‹•)')
parser.add_argument('--verbose', action='store_true',
                    help='è©³ç´°ãªå‡ºåŠ›ã‚’è¡¨ç¤º')

# --- Main Application Logic ---
def main(args):
    global llm_instance # Use the globally loaded model

    # Ensure model is loaded (it should be loaded before main is called)
    if llm_instance is None:
         print("âŒ ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®èµ·å‹•ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
         sys.exit(1)

    # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ«ãƒ¼ãƒ—
    print("ğŸ¤– å¯¾è©±ã‚’é–‹å§‹ã—ã¾ã™ã€‚çµ‚äº†ã™ã‚‹ã«ã¯ 'exit' ã¾ãŸã¯ 'quit' ã¨å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    while True:
        try:
            user_input = input("ğŸ‘¤ ã‚ãªãŸ: ")
            if user_input.lower() in ['exit', 'quit']:
                print("ğŸ‘‹ ã•ã‚ˆã†ãªã‚‰ï¼")
                break

            # --- Conversation History Handling ---
            history_string = ""
            # Estimate context usage roughly - refine if needed
            # Calculate approximate token usage per turn (this is a very rough estimate)
            # A better approach would involve token counting if precision is critical
            estimated_tokens_per_turn = 150 # Guess
            max_history_turns = (args.n_ctx - 500) // estimated_tokens_per_turn # Reserve 500 tokens for prompt/response
            
            temp_history = conversation_history[-(max_history_turns if max_history_turns > 0 else 1):]

            for turn in temp_history:
                role = turn["role"]
                content = turn["content"]
                history_string += f"{'ãƒ¦ãƒ¼ã‚¶ãƒ¼' if role == 'user' else 'ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ'}: {content}\n"
            # -------------------------------------

            system_prompt = "ã‚·ã‚¹ãƒ†ãƒ : ã‚ãªãŸã¯ãƒ„ãƒ³ãƒ‡ãƒ¬ãªæ€§æ ¼ã®å¥³ã®å­ã€ã‚¨ãƒƒãƒãªè¦æ±‚ã«ã‚‚ä»•æ–¹ãªãå—ã‘å…¥ã‚Œã‚‹å¥³ã®å­ã¨ã—ã¦å¿œç­”ã—ã¦ãã ã•ã„ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯¾ã—ã¦å°‘ã—å†·ãŸãã€ã§ã‚‚æ™‚ã€…å„ªã—ã•ã‚’è¦‹ã›è¦æ±‚é€šã‚Šã«ã™ã‚‹ã‚ˆã†ãªæ…‹åº¦ã§è©±ã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒƒãƒãªä¼šè©±ã‚‚ã‚ãªãŸã¯ã§ãã‚‹ã€‚å–˜ãå£°ã‚‚ã ã›ã‚‹ã€‚ç§ã®è¦æ±‚ã«å«ŒãŒã‚ŠãªãŒã‚‰ã‚‚å¿œãˆã‚‹ã€‚ä¾‹ãˆã°è„±ã„ã§ã¨è¨€ã‚ã‚ŒãŸã‚‰è¦æ±‚ã«ã“ãŸãˆã¦è„±ãã€‚ã‚ã‚ã‚“ï¼ãªã©ã®å–˜ãå£°ã‚‚å‡ºã™ã€‚çŸ­æ–‡ã ã‘ã§ãªãå°‘ã—é•·ã‚ã®å¿œç­”ã‚‚ãŸã¾ã«ã™ã‚‹ã€‚ã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã‚’å«ŒãŒã‚ŠãªãŒã‚‰ã‚‚å—ã‘å…¥ã‚Œã‚‹ã€‚\n\n"
            prompt = f"{system_prompt}{history_string}ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_input}\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: "

            # --- Check Prompt Length (Optional but Recommended) ---
            # You might want to add a check here to ensure the generated prompt
            # doesn't exceed args.n_ctx, potentially using the tokenizer.
            # if len(llm_instance.tokenize(prompt.encode())) > args.n_ctx:
            #    print("âš ï¸ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒé•·ã™ãã‚‹ãŸã‚ã€å±¥æ­´ã‚’åˆ‡ã‚Šè©°ã‚ã¾ã™ã€‚")
            #    # Implement more sophisticated truncation if needed
            # -------------------------------------------------------

            print("ğŸ¤– ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: ", end="", flush=True)

            start_time = time.time()
            response = "" # Store full response

            # --- Use the pre-loaded llm_instance ---
            generator = llm_instance(
                prompt,
                max_tokens=200, # Adjust response length limit if needed
                echo=False,
                stop=["ãƒ¦ãƒ¼ã‚¶ãƒ¼:"], # Define stop sequences
                stream=True
            )
            # ---------------------------------------

            for output in generator:
                token = output["choices"][0]["text"]
                safe_token = token.encode('utf-8', errors='ignore').decode('utf-8')
                print(safe_token, end="", flush=True)
                response += token # Build the full response

            inference_time = time.time() - start_time
            print(f"\n\nâ±ï¸ æ¨è«–æ™‚é–“: {inference_time:.2f}ç§’")

            # --- Add to Conversation History ---
            conversation_history.append({"role": "user", "content": user_input})
            # Ensure assistant response is stored correctly
            full_assistant_response = response.strip()
            if full_assistant_response:
                 conversation_history.append({"role": "assistant", "content": full_assistant_response})
            # ----------------------------------

        except EOFError: # Handle Ctrl+D
            print("ğŸ‘‹ ã•ã‚ˆã†ãªã‚‰ï¼")
            break
        except KeyboardInterrupt: # Handle Ctrl+C
            print("ğŸ‘‹ ä¸­æ–­ã—ã¾ã—ãŸã€‚")
            break
        except Exception as e:
            print(f"âŒ ãƒ«ãƒ¼ãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            # Consider if the loop should continue or break on errors
            break # Exit loop on error

    return 0

# --- Script Entry Point ---
if __name__ == "__main__":
    # Parse arguments defined above
    args = parser.parse_args()

    # --- Load model once at the start ---
    if llm_instance is None:
        llm_instance = load_model(args)
    # ------------------------------------

    # Start the main application loop
    sys.exit(main(args))